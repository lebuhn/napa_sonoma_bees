#Match projections
spdf.mrc <- spTransform(spdf, CRS(projection(ecoregion.rg)))
#Combine datasets
o = over(spdf.mrc,ecoregion.rg)
gsp.spatial <- cbind(spdf.mrc, o)
head(gsp.spatial)
# create map
map1 = plot(ecoregion.rg, col = "topo.colors"(ecoregion.rg@data$DIVISION))
points(spdf.mrc, col="magenta")
# load cover data
NLCD <- raster("/Volumes/BLUE DRIVE/nlcd_2011_landcover_2011_edition_2014_10_10/nlcd_2011_landcover_2011_edition_2014_10_10.img")
# load site data
sites <- lq.data
coords <- sites[, c("long_esri_wgs84_1", "lat_esri_wgs84_1")]
#convert lat/lon to appropriate projection
names(coords) <- c("x", "y")
coordinates(coords) <- ~x + y
proj4string(coords) <- CRS("+proj=longlat +ellps=WGS84 +datum=WGS84")
crs_args <- NLCD@crs@projargs
sites_transformed <- spTransform(coords, CRS(crs_args))
#extract land cover data for each point, given buffer size
Landcover <- extract(NLCD, sites_transformed, buffer=1000)
# summarize each site's data by proportion of each cover type
summ <- lapply(Landcover, function(x){
prop.table(table(x))
}
)
# generate land cover number to name conversions
num.codes <- unique(unlist(Landcover))
cover.names <- NLCD@data@attributes[[1]]$NLCD.2011.Land.Cover.Class[num.codes + 1]
levels(cover.names)[1] <- NA # first level is ""
conversions <- data.frame(num.codes, cover.names)
conversions <- na.omit(conversions)
conversions <- conversions[order(conversions$num.codes),]
# convert to data frame
mydf <- data.frame(id = rep(sites$new_nid_nosp, lapply(summ, length)),
cover = names(unlist(summ)),
percent = unlist(summ)
)
# create cover name column
mydf$cover2 <- mydf$cover
levels(mydf$cover2) <- conversions$cover.names
# write output
write.csv(mydf, "~/Documents/4. Great Sunflower/1.DATA/GSP/GreatSunflower/Data/Bees/2013/DariyaGeocode/GDB_and_final_table/2011_cover_1000_m_buffer.csv")
#=====================================
# Script to actually extract the data at various buffer distances
# parallelized for speed (one core per year)
#=====================================
library(doMC)
# years <- c( 2011)
# nyears <- length(years)
# registerDoMC(nyears)
# # input vector of distances (in meters)
# buffer_distances <- c(1000)
# foreach (i=1:nyears) %dopar% {
# for (j in buffer_distances){
# extract_cover(year = years[i], buffer=j)
# }
# }
extract_cover(year=2011,buffer=1000)
plot(NLCD)
points(sites_transformed)
#=====================================
#Function to add  PRISM precipitation raster data
#=====================================
#load data
precip <- raster("~/Documents/4. Great Sunflower/1.DATA/GSP/GreatSunflower/Data/Climate/PRISM_ppt_30yr_normal_4kmM2_annual.png")
#look at raster attributes
precip
#set min and max values for rasters attributes
precip <- setMinMax(precip)
precip
# view coordinate reference system
precip@crs
#view raster extent
precip@extent
#match the CRS
crs(precip)<- CRS("+proj=longlat +ellps=WGS84 +datum=WGS84")
#plot the raster
plot(precip, main= "Prism precip data 30 Year")
#extract precip data for each point, given buffer size
Precip <- extract(precip, sites_transformed, buffer=1000)
# summarize each site's data
summ2 <- lapply(precip, function(x){
max(x)
}
)
# convert to data frame
mydf2 <- data.frame(id = rep(sites$new_nid_nosp, precip)  )
write.csv(mydf, "~/Documents/4. Great Sunflower/1.DATA/GSP/GreatSunflower/Data/Bees/2013/DariyaGeocode/GDB_and_final_table/ppt_30yr_normal_4km2_annual.csv")
plot(precip)
points(sites_transformed)
min(precip)
lq.2015.data <- smartbind (New.data, lq.subset)
#=====================================
#Code for using shapefiles and raster files with the core GSP data set
# Gretchen LeBuhn (lebuhn at sfsu.edu)
# January 2016
# updated November 2016 to add data through 2015 and move things to dropbox
#=====================================
#load the raster, sp, and rgdal packages
library(raster)
library(sp)
library(rgdal)
library(RColorBrewer)
library(rgeos)
library(stringr)
#setwd("~/Documents/4. Great Sunflower/1.DATA/GSP/GreatSunflower/Data/Bees/2013/DariyaGeocode/GDB_and_final_table/") #GLBMac
#setwd("https://github.com/lebuhn") #iplant repository
setwd("~/Dropbox/GSP2016") #dropbox
#=====================================
#=====READ IN GSP DATA
#=====================================
source("~/Dropbox/GSP2016/R-scripts/GSP_data_creation.R")
#=====================================
#Add shapefile data
#=====================================
#read shapefile
ogrInfo("./data/eco-us-shp/", "eco_us")
#  read the .prj file
#= This file is from http://www.fs.fed.us/rm/ecoregions/images/maps/ecoregions-united-states.jpg on 12/
ecoregion.rg <- readOGR("./data/eco-us-shp/", "eco_us")
print(proj4string(ecoregion.rg))
class(ecoregion.rg$DIVISION)
spplot(ecoregion.rg, "DIVISION",
col.regions=topo.colors(nlevels(ecoregion.rg$DIVISION)))
#Create spatial data frames for points
xy <- lq.2015.data[,c("long_esri_wgs84_1","lat_esri_wgs84_1")]
spdf <- SpatialPointsDataFrame(coords = xy, data = lq.2015.data,
proj4string = CRS("+proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0"))
#Match projections
spdf.mrc <- spTransform(spdf, CRS(projection(ecoregion.rg)))
#Combine datasets
o = over(spdf.mrc,ecoregion.rg)
gsp.spatial <- cbind(spdf.mrc, o)
head(gsp.spatial)
# create map
map1 = plot(ecoregion.rg, col = "topo.colors"(ecoregion.rg@data$DIVISION))
points(spdf.mrc, col="magenta")
# load site data
sites <- lq.2015.data
coords <- sites[, c("long_esri_wgs84_1", "lat_esri_wgs84_1")]
#convert lat/lon to appropriate projection
names(coords) <- c("x", "y")
coordinates(coords) <- ~x + y
proj4string(coords) <- CRS("+proj=longlat +ellps=WGS84 +datum=WGS84")
crs_args <- NLCD@crs@projargs
sites_transformed <- spTransform(coords, CRS(crs_args))
url<-"http://gisdata.usgs.gov/TDDS/DownloadFile.php?TYPE=nlcd2011&ORIG=SBDDG&FNAME=nlcd_2011_landcover_2011_edition_2014_10_10.zip"
mydir<-"~/Dropbox/GSP2016/junk"
temp<-tempfile(tmpdir=mydir, fileext=".zip")
download.file(url, temp)
unzip(temp, exdir=mydir)
unlink(temp) #delete the zip file
mydir<-"~/Dropbox/GSP2016/junk"
temp<-tempfile(tmpdir=mydir, fileext=".zip")
download.file(url, temp)
unzip(temp, exdir=mydir)
unlink(temp) #delete the zip file
fpath<-list.files(path = mydir, full.names = TRUE, pattern = "nlcd_2011_landcover_2011_edition_2014_10_10/nlcd_2011_landcover_2011_edition_2014_10_10")
fpath<-gsub("/", "\\\\", fpath)
landusepath<-paste(fpath, "raster\\nlcd_2011_landcover_2011_edition_2014_10_10/nlcd_2011_landcover_2011_edition_2014_10_10.img", sep="\\")
NLCD<-raster(landusepath)
plot(NLCD, axes=FALSE)
landusepath<-paste(fpath, "raster\\nlcd_2011_landcover_2011_edition_2014_10_10/nlcd_2011_landcover_2011_edition_2014_10_10.img", sep="\\")
landusepath<-paste(fpath, "\nlcd_2011_landcover_2011_edition_2014_10_10/nlcd_2011_landcover_2011_edition_2014_10_10.img", sep="\\")
NLCD<-raster(landusepath)
landusepath<-paste(fpath, "\nlcd_2011_landcover_2011_edition_2014_10_10/nlcd_2011_landcover_2011_edition_2014_10_10.img")
NLCD<-raster(landusepath)
NLCD<-raster("~/nlcd_2011_landcover_2011_edition_2014_10_10/nlcd_2011_landcover_2011_edition_2014_10_10.img")
NLCD<-raster(mydir,"~/nlcd_2011_landcover_2011_edition_2014_10_10/nlcd_2011_landcover_2011_edition_2014_10_10.img")
NLCD<-raster("~/Dropbox/GSP2016/junk/nlcd_2011_landcover_2011_edition_2014_10_10/nlcd_2011_landcover_2011_edition_2014_10_10.img")
plot(NLCD, axes=FALSE)
# load site data
sites <- lq.2015.data
coords <- sites[, c("long_esri_wgs84_1", "lat_esri_wgs84_1")]
#convert lat/lon to appropriate projection
names(coords) <- c("x", "y")
coordinates(coords) <- ~x + y
proj4string(coords) <- CRS("+proj=longlat +ellps=WGS84 +datum=WGS84")
crs_args <- NLCD@crs@projargs
sites_transformed <- spTransform(coords, CRS(crs_args))
#extract land cover data for each point, given buffer size
Landcover <- extract(NLCD, sites_transformed, buffer=1000)
# summarize each site's data by proportion of each cover type
summ <- lapply(Landcover, function(x){
prop.table(table(x))
}
)
# generate land cover number to name conversions
num.codes <- unique(unlist(Landcover))
cover.names <- NLCD@data@attributes[[1]]$NLCD.2011.Land.Cover.Class[num.codes + 1]
levels(cover.names)[1] <- NA # first level is ""
conversions <- data.frame(num.codes, cover.names)
conversions <- na.omit(conversions)
conversions <- conversions[order(conversions$num.codes),]
# convert to data frame
mydf <- data.frame(id = rep(sites$new_nid_nosp, lapply(summ, length)),
cover = names(unlist(summ)),
percent = unlist(summ)
)
# create cover name column
mydf$cover2 <- mydf$cover
levels(mydf$cover2) <- conversions$cover.names
# write output
write.csv(mydf, "~/Dropbox/GSP2016/data/2011_cover_1000_m_buffer.csv")
#=====================================
# Script to actually extract the data at various buffer distances
# parallelized for speed (one core per year)
#=====================================
library(doMC)
# years <- c( 2011)
# nyears <- length(years)
# registerDoMC(nyears)
# # input vector of distances (in meters)
# buffer_distances <- c(1000)
# foreach (i=1:nyears) %dopar% {
# for (j in buffer_distances){
# extract_cover(year = years[i], buffer=j)
# }
# }
extract_cover(year=2011,buffer=1000)
plot(NLCD)
points(sites_transformed)
#=====================================
#Function to add  PRISM precipitation raster data
#=====================================
#load data
precip <- raster("~/Documents/4. Great Sunflower/1.DATA/GSP/GreatSunflower/Data/Climate/PRISM_ppt_30yr_normal_4kmM2_annual.png")
#look at raster attributes
precip
#set min and max values for rasters attributes
precip <- setMinMax(precip)
precip
# view coordinate reference system
precip@crs
#view raster extent
precip@extent
#match the CRS
crs(precip)<- CRS("+proj=longlat +ellps=WGS84 +datum=WGS84")
#plot the raster
plot(precip, main= "Prism precip data 30 Year")
#extract precip data for each point, given buffer size
Precip <- extract(precip, sites_transformed, buffer=1000)
# summarize each site's data
summ2 <- lapply(precip, function(x){
max(x)
}
)
# convert to data frame
mydf2 <- data.frame(id = rep(sites$new_nid_nosp, precip)  )
write.csv(mydf, "~/Documents/4. Great Sunflower/1.DATA/GSP/GreatSunflower/Data/Bees/2013/DariyaGeocode/GDB_and_final_table/ppt_30yr_normal_4km2_annual.csv")
plot(precip)
points(sites_transformed)
min(precip)
library(raster)
library(doMC)
library(raster)
# years <- c( 2011)
# nyears <- length(years)
# registerDoMC(nyears)
# # input vector of distances (in meters)
# buffer_distances <- c(1000)
# foreach (i=1:nyears) %dopar% {
# for (j in buffer_distances){
# extract_cover(year = years[i], buffer=j)
# }
# }
extract_cover(year=2011,buffer=1000)
plot(NLCD)
points(sites_transformed)
#=====================================
#Function to add  PRISM precipitation raster data
#=====================================
#load data
precip <- raster("~/Documents/4. Great Sunflower/1.DATA/GSP/GreatSunflower/Data/Climate/PRISM_ppt_30yr_normal_4kmM2_annual.png")
#look at raster attributes
precip
#set min and max values for rasters attributes
precip <- setMinMax(precip)
precip
# view coordinate reference system
precip@crs
#view raster extent
precip@extent
#match the CRS
crs(precip)<- CRS("+proj=longlat +ellps=WGS84 +datum=WGS84")
#plot the raster
plot(precip, main= "Prism precip data 30 Year")
#extract precip data for each point, given buffer size
Precip <- extract(precip, sites_transformed, buffer=1000)
# summarize each site's data
summ2 <- lapply(precip, function(x){
max(x)
}
)
# convert to data frame
mydf2 <- data.frame(id = rep(sites$new_nid_nosp, precip)  )
write.csv(mydf, "~/Documents/4. Great Sunflower/1.DATA/GSP/GreatSunflower/Data/Bees/2013/DariyaGeocode/GDB_and_final_table/ppt_30yr_normal_4km2_annual.csv")
plot(precip)
points(sites_transformed)
min(precip)
write.csv(mydf, "~/Dropbox/GSP2016/data/ppt_30yr_normal_4km2_annual.csv")
plot(precip)
points(sites_transformed)
min(precip)
# load site data
sites <- lq.2015.data
coords <- sites[, c("long_esri_wgs84_1", "lat_esri_wgs84_1")]
#convert lat/lon to appropriate projection
names(coords) <- c("x", "y")
coordinates(coords) <- ~x + y
proj4string(coords) <- CRS("+proj=longlat +ellps=WGS84 +datum=WGS84")
crs_args <- NLCD@crs@projargs
sites_transformed <- spTransform(coords, CRS(crs_args))
#extract land cover data for each point, given buffer size
Landcover <- extract(NLCD, sites_transformed, buffer=1000)
# convert to data frame
mydf <- data.frame(id = rep(sites$userID, lapply(summ, length)),
cover = names(unlist(summ)),
percent = unlist(summ)
)
# create cover name column
mydf$cover2 <- mydf$cover
levels(mydf$cover2) <- conversions$cover.names
# write output
write.csv(mydf, "~/Dropbox/GSP2016/data/2011_cover_1000_m_buffer.csv")
View(All.data)
head(sites)
tail(sites)
# convert to data frame
mydf <- data.frame(id = rep(sites$username, lapply(summ, length)),
cover = names(unlist(summ)),
percent = unlist(summ)
)
# create cover name column
mydf$cover2 <- mydf$cover
levels(mydf$cover2) <- conversions$cover.names
# write output
write.csv(mydf, "~/Dropbox/GSP2016/data/2011_cover_1000_m_buffer.csv")
View(All.data)
tail (All.data)
plot(precip)
min(precip)
points(sites_transformed)
names (lq.2015.data)
head (lq.2015.data)
tail (lq.2015.data)
head(lq.2015.data)
head(New.data)
#===========
#Code for importing and merging GSP data from exported .csv files
#==============
library(plyr)
library(gtools)
datadir="~/Dropbox/GSP2016/data/"
#=====READ IN GSP DATA AND REDUCE TO LEMON QUEEN SAMPLES ONLY
#=====================================
New.data <-read.csv(paste(datadir,"GSP.LQ.2013.2016.csv",sep=""), header = TRUE, stringsAsFactors = F)
GSP.data = read.csv(paste(datadir,"clean_GSP_data.csv",sep=""), header = TRUE, stringsAsFactors = F)
lq.data <- GSP.data[grep("LEMON QUEEN SUNFLOWER", GSP.data$plant_name, fixed = TRUE), ]
names(lq.data)
names(New.data)
#==========Simplify lemon queen data set
lq.data <- rename(lq.data, c(bee_names="pollinatorName", user_id="userID", name="username"))
lq.data <- within(lq.data, pollinatorName <- ifelse(is.na(pollinatorName), bee_name, pollinatorName))
head(lq.data)
# select variables
myvars <- c("userID", "username", "time", "locationID", "new_nid_nosp",  "new_nid", "month", "day", "year",  "street", "garden_city", "garden_province", "garden_postal_code", "ecoregion", "garden_country", "lat_esri_wgs84_1", "long_esri_wgs84_1", "matched_address",  "pollinatorName", "flowers_on_plant", "plant_name", "new_maxObs" , "new_maxObs_Sp", "COUNT_perHourFlower_Sp", "COUNT_perHourFlower","sample_length" )
lq.subset <- lq.data[myvars]
#===============Match up variable names and data
New.data <- rename(New.data, c(locationNID="locationID", observationNID="new_nid", plantName="plant_name", flowerCount="flowers_on_plant", Latitude="lat_esri_wgs84_1", Longitude="long_esri_wgs84_1", bphf="COUNT_perHourFlower_Sp", bph="new_maxObs"))
COUNT_perHourFlower<-aggregate( COUNT_perHourFlower_Sp ~ userID + date + time , New.data , sum )
New.data <- merge(x=COUNT_perHourFlower, y=New.data, by= c("userID","date", "time"), all.y = TRUE)
# New.data$date<-as.Date(New.data$date,"%m/%d/%Y")
# New.data$year = format(New.data$date,'%Y')
# New.data$month = as.numeric(format(New.data$date, format = "%m"))
# New.data$day = as.numeric(format(New.data$date, format = "%d"))
New.data <- rename(New.data, c(COUNT_perHourFlower_Sp.x = "COUNT_perHourFlower", COUNT_perHourFlower_Sp.y = "COUNT_perHourFlower_Sp"))
head(New.data)
#===  Convert sample time to minutes
New.data$sample_length=New.data$sampletime.hrs * 60
#=========MERGE GSP DATA AND NEW DATA
names (New.data)
names(lq.subset)
lq.2015.data <- smartbind (New.data, lq.subset)
#
# write.csv(lq.2015.data, "~/Dropbox/GSP2016/data/All.data.csv")
#  read the .prj file
#= This file is from http://www.fs.fed.us/rm/ecoregions/images/maps/ecoregions-united-states.jpg on 12/
ecoregion.rg <- readOGR("./data/eco-us-shp/", "eco_us")
print(proj4string(ecoregion.rg))
class(ecoregion.rg$DIVISION)
spplot(ecoregion.rg, "DIVISION",
col.regions=topo.colors(nlevels(ecoregion.rg$DIVISION)))
#load the raster, sp, and rgdal packages
library(raster)
library(sp)
library(rgdal)
library(RColorBrewer)
library(rgeos)
library(stringr)
#setwd("~/Documents/4. Great Sunflower/1.DATA/GSP/GreatSunflower/Data/Bees/2013/DariyaGeocode/GDB_and_final_table/") #GLBMac
#setwd("https://github.com/lebuhn") #iplant repository
setwd("~/Dropbox/GSP2016") #dropbox
print(proj4string(ecoregion.rg))
class(ecoregion.rg$DIVISION)
spplot(ecoregion.rg, "DIVISION",
col.regions=topo.colors(nlevels(ecoregion.rg$DIVISION)))
#Create spatial data frames for points
xy <- lq.2015.data[,c("long_esri_wgs84_1","lat_esri_wgs84_1")]
spdf <- SpatialPointsDataFrame(coords = xy, data = lq.2015.data,
proj4string = CRS("+proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0"))
#Match projections
spdf.mrc <- spTransform(spdf, CRS(projection(ecoregion.rg)))
#Combine datasets
o = over(spdf.mrc,ecoregion.rg)
gsp.spatial <- cbind(spdf.mrc, o)
head(gsp.spatial)
# create map
map1 = plot(ecoregion.rg, col = "topo.colors"(ecoregion.rg@data$DIVISION))
points(spdf.mrc, col="magenta")
#=====================================
#Function to add  PRISM precipitation raster data
#=====================================
#load data
precip <- raster("~/Dropbox/GSP2016/data/PRISM_ppt_30yr_normal_4kmM2_annual.png")
#look at raster attributes
precip
#set min and max values for rasters attributes
precip <- setMinMax(precip)
precip
# view coordinate reference system
precip@crs
#view raster extent
precip@extent
#match the CRS
crs(precip)<- CRS("+proj=longlat +ellps=WGS84 +datum=WGS84")
#plot the raster
plot(precip, main= "Prism precip data 30 Year")
#extract precip data for each point, given buffer size
Precip <- extract(precip, sites_transformed, buffer=1000)
load the raster, sp, and rgdal packages
library(raster)
library(sp)
library(rgdal)
library(RColorBrewer)
library(rgeos)
library(stringr)
#setwd("~/Documents/4. Great Sunflower/1.DATA/GSP/GreatSunflower/Data/Bees/2013/DariyaGeocode/GDB_and_final_table/") #GLBMac
#setwd("https://github.com/lebuhn") #iplant repository
setwd("~/Dropbox/GSP2016") #dropbox
#  read the .prj file
#= This file is from http://www.fs.fed.us/rm/ecoregions/images/maps/ecoregions-united-states.jpg on 12/
ecoregion.rg <- readOGR("./data/eco-us-shp/", "eco_us")
print(proj4string(ecoregion.rg))
class(ecoregion.rg$DIVISION)
spplot(ecoregion.rg, "DIVISION",
col.regions=topo.colors(nlevels(ecoregion.rg$DIVISION)))
#Create spatial data frames for points
xy <- lq.2015.data[,c("long_esri_wgs84_1","lat_esri_wgs84_1")]
spdf <- SpatialPointsDataFrame(coords = xy, data = lq.2015.data,
proj4string = CRS("+proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0"))
#Match projections
spdf.mrc <- spTransform(spdf, CRS(projection(ecoregion.rg)))
#Combine datasets
o = over(spdf.mrc,ecoregion.rg)
gsp.spatial <- cbind(spdf.mrc, o)
head(gsp.spatial)
# create map
map1 = plot(ecoregion.rg, col = "topo.colors"(ecoregion.rg@data$DIVISION))
points(spdf.mrc, col="magenta")
===========================================================================
# Program to extract NLCD data in R
# inputs: year, buffer size (in meters), point data file,
#   cover data directory, and output file directory
# output: csv file with site id, cover type, and % in buffer
#========================================================================
# extract_cover <- function(year, buffer,
# point_d = "sites.csv",
# data_dir="/BLUE DRIVE/",
# write_dir="extracted"){
# require(raster)
# require(rgdal)
# require(stringr)
# load cover data
# data location
url<-"http://gisdata.usgs.gov/TDDS/DownloadFile.php?TYPE=nlcd2011&ORIG=SBDDG&FNAME=nlcd_2011_landcover_2011_edition_2014_10_10.zip"
mydir<-"~/Dropbox/GSP2016/junk"
temp<-tempfile(tmpdir=mydir, fileext=".zip")
download.file(url, temp)
# load site data
sites <- lq.2015.data
coords <- sites[, c("long_esri_wgs84_1", "lat_esri_wgs84_1")]
#convert lat/lon to appropriate projection
names(coords) <- c("x", "y")
coordinates(coords) <- ~x + y
proj4string(coords) <- CRS("+proj=longlat +ellps=WGS84 +datum=WGS84")
crs_args <- NLCD@crs@projargs
sites_transformed <- spTransform(coords, CRS(crs_args))
#extract land cover data for each point, given buffer size
Landcover <- extract(NLCD, sites_transformed, buffer=1000)
precip <- raster("~/Dropbox/GSP2016/data/PRISM_ppt_30yr_normal_4kmM2_annual.png")
#look at raster attributes
precip
#set min and max values for rasters attributes
precip <- setMinMax(precip)
precip
# view coordinate reference system
precip@crs
#view raster extent
precip@extent
